{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# used for manipulating directory paths\nimport os\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Plotting library\nfrom matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces\n# tells matplotlib to embed plots within the notebook\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCostMulti(X, y, theta):\n    \"\"\"\n    Compute cost for linear regression with multiple variables.\n    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    Returns\n    -------\n    J : float\n        The value of the cost function. \n    \n    Instructions\n    ------------\n    Compute the cost of a particular choice of theta. You should set J to the cost.\n    \"\"\"\n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # You need to return the following variable correctly\n    J = 0\n    \n    # ======================= YOUR CODE HERE ===========================\n    \n\n    h = np.dot(X, theta)\n    \n    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))\n    # ==================================================================\n    return J\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradientDescentMulti(X, y, theta, alpha, num_iters):\n    \"\"\"\n    Performs gradient descent to learn theta.\n    Updates theta by taking num_iters gradient steps with learning rate alpha.\n        \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    alpha : float\n        The learning rate for gradient descent. \n    \n    num_iters : int\n        The number of iterations to run gradient descent. \n    \n    Returns\n    -------\n    theta : array_like\n        The learned linear regression parameters. A vector of shape (n+1, ).\n    \n    J_history : list\n        A python list for the values of the cost function after each iteration.\n    \n    Instructions\n    ------------\n    Peform a single gradient step on the parameter vector theta.\n\n    While debugging, it can be useful to print out the values of \n    the cost function (computeCost) and gradient here.\n    \"\"\"\n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # make a copy of theta, which will be updated by gradient descent\n    theta = theta.copy()\n    \n    J_history = []\n    \n    for i in range(num_iters):\n        # ======================= YOUR CODE HERE ==========================\n        \n        \n\n        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)\n        # =================================================================\n        \n        # save the cost J in every iteration\n        J_history.append(computeCostMulti(X, y, theta))\n    \n    return theta, J_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\nimport math\n\n\n#Reading the csv and dropping all nan attributes\n\ndata = pd.read_csv(os.path.join('house_prices_data_training_data.csv'))\ndata.dropna(inplace=True)\ndata.shape\n\nfeatures = data\nlabel = data['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features.drop(columns = ['date','id','zipcode','price','condition','yr_built','long','sqft_lot15'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['bedrooms2'] = features['bedrooms']**2\nfeatures['sqft_basement2'] = features['sqft_basement']**2\nfeatures['lat2'] = features['lat']**2\nfeatures['view3'] = features['view']**3\nfeatures['bathrooms4'] = features['bathrooms']**4\nfeatures['sqft_living154'] = features['sqft_living15']**4\nfeatures['grade5'] = features['grade']**5\nfeatures['sqft_above5'] = features['sqft_above']**5\nfeatures['sqft_living6'] = features['sqft_living']**6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features = data.drop(columns = ['price','date','id'])\nprint(features['bedrooms'])\nfeatures = features.to_numpy()\nlabel = label.to_numpy()\n\nlen(features[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#normalize feature values\nmu = np.mean(features, axis = 0)\nsigma = np.std(features, axis = 0)\nnormalizedFeatures = (features - mu) / sigma\n\nfeatures = np.concatenate([np.ones((label.size, 1)), normalizedFeatures], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split into 60-20-20 sets\n\nfeaturesTrain = features[0:10800,:]\nlabelTrain =  label[0:10800]\nfeaturesCross = features[10800:14400,:]\nlabelCross = label[10800:14400]\nfeaturesTest = features[14400:17999,:]\nlabelTest = label[14400:17999]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Theta initialization\n#Without any mutations\nprint(len(featuresTrain[0]))\ntheta = np.zeros(featuresTrain[:,0:21].shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Theta initialization\n#With dropping\nprint(len(featuresTrain[0]))\ntheta = np.zeros(featuresTrain[:,0:13].shape[1])\nprint(len(theta))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Theta initialization\n#With dropping & polys\nprint(len(featuresTrain[0]))\ntheta = np.zeros(featuresTrain[:,0:22].shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = 0.1\nnum_iters = 400\n\n\nprint(featuresTrain)\nprint(labelTrain)\ntheta, J_history = gradientDescentMulti(featuresTrain[:,0:22], labelTrain, theta, alpha, num_iters)\n\n\n# Plot the convergence graph\npyplot.plot(np.arange(len(J_history)), J_history, lw=2)\npyplot.xlabel('Number of iterations')\npyplot.ylabel('Cost J')\n\n# Display the gradient descent's result\nprint('theta computed from gradient descent: {:s}'.format(str(theta)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"computeCostMulti(featuresCross[:,0:22],labelCross,theta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"computeCostMulti(featuresTest[:,0:22],labelTest,theta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#K-fold\n\n# K will be set to 5 \n#so we will divide the dataset to 5 sample sets \nkFold = 0.25\nk1 = features[0:math.ceil(len(features)*kFold)]\nk2 = features[math.ceil(len(features)*kFold) : math.ceil(len(features)*kFold *2)]\nk3 = features[math.ceil(len(features)*kFold *2) : math.ceil(len(features)*kFold *3)]\nk4 = features[math.ceil(len(features)*kFold *3): math.ceil(len(features))]\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"alpha = 0.1\nnum_iters = 500\ntheta = np.zeros(k1.shape[1])\ntheta1,J_history1 = gradientDescentMulti(k2, label[4500:9000], theta, alpha, num_iters)\ntheta2,J_history2 = gradientDescentMulti(k3, label[9000:13500], theta, alpha, num_iters)\ntheta3,J_history3 = gradientDescentMulti(k4, label[13500:18000], theta, alpha, num_iters)\n\n\n# Plot the convergence graph\npyplot.plot(np.arange(len(J_history3)), J_history3, lw=2)\npyplot.xlabel('Number of iterations')\npyplot.ylabel('Cost J')\n\n# Display the gradient descent's result\nprint('theta computed from gradient descent: {:s}'.format(str(theta3)))\n\n\n# saving the last values of theta to be used in the next training iteration\ntheta=theta3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('the test sample cost = ', computeCostMulti(S1,Y[0:3600],theta4))","execution_count":21,"outputs":[{"name":"stdout","output_type":"stream","text":"the test sample cost =  21184492468.779766\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Regularization\n\n## Creating Regularization functions\n\ndef gradientDescentRegularization(X, y, theta, alpha, num_iters,lambda_):\n    \n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # make a copy of theta, which will be updated by gradient descent\n    theta = theta.copy()\n    \n    J_history = []\n    \n    for i in range(num_iters):\n\n        theta = theta - (alpha / m) * (((np.dot(X, theta) - y).dot(X)) + (lambda_*theta))\n        # =================================================================\n        \n        # save the cost J in every iteration\n        J_history.append(computeCostMulti(X, y, theta))\n    \n    return theta, J_history\n\ndef computeCostRegularization(X, y, theta,lambda_):\n    \n    # Initialize some useful values\n    m = y.shape[0] # number of training examples\n    \n    # You need to return the following variable correctly\n    J = 0        \n    h = np.dot(X, theta)\n    \n    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y)) + ((lambda_/2*m)*np.sum(np.square(theta)))\n    return J\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# training each model of different degrees from 1 to 6 with all the set of lambdas\n\nlambda_ = [0,0.01,0.05,0.1,0.5,1,5,10]\n\nfor i in range(len(lambda_)):\n    \n    theta = np.zeros(X_train[:,0:17].shape[1])\n\n    theta, J_history = gradientDescentRegularization(X_train[:,0:17], Y_train, theta, alpha, num_iters,lambda_[i])\n    \n    print(computeCostRegularization(X_cross[:,0:17],Y_cross,theta,lambda_[i]))","execution_count":30,"outputs":[{"name":"stdout","output_type":"stream","text":"18937756247.515877\n6077063518959.113\n30309337291646.06\n60599163639835.836\n302897141323682.6\n605718038452755.8\n3026223819932054.0\n6046710316890518.0\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}