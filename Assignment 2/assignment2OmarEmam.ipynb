{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Programming Exercise 2: Logistic Regression\n\n## Introduction\n\nIn this exercise, you will implement logistic regression and apply it to two different datasets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# used for manipulating directory paths\nimport os\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Plotting library\nfrom matplotlib import pyplot\n\n# Optimization module in scipy\nfrom scipy import optimize\n\n# library written for this exercise providing additional functions for assignment submission, and others\nimport utils\n\nimport math\n\n# tells matplotlib to embed plots within the notebook\n%matplotlib inline","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 Logistic Regression\n\nIn this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and\nyou want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions\ndecision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. \n\nThe following cell will load the data and corresponding labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.loadtxt(os.path.join('ex2data1.txt'), delimiter=',')\nnp.random.shuffle(data)\nfeatures, label = data[:, 0:2], data[:, 2]\nfeatures = np.concatenate([np.ones((data.shape[0], 1)), features], axis=1)\n","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features = np.zeros((features.shape[0],5))\npoly_features[:,0:3] = features[:,0:3]\npoly_features[:,4] = features[:,2]**2\npoly_features[:,3] = features[:,1]**3","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresTrain = poly_features[0:math.ceil(len(poly_features)*0.6),:]\nlabelTrain = label[0:math.ceil(len(label)*0.6)]\nfeaturesCross = poly_features[math.ceil(len(poly_features)*0.6):math.ceil(len(poly_features)*0.8),:]\nlabelCross = label[math.ceil(len(label)*0.6):math.ceil(len(label)*0.8)]\nfeaturesTest = poly_features[math.ceil(len(poly_features)*0.8):math.ceil(len(poly_features)),:]\nlabelTest = label[math.ceil(len(label)*0.8):math.ceil(len(label))]","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"\n    Compute sigmoid function given the input z.\n    \n    Parameters\n    ----------\n    z : array_like\n        The input to the sigmoid function. This can be a 1-D vector \n        or a 2-D matrix. \n    \n    Returns\n    -------\n    g : array_like\n        The computed sigmoid function. g has the same shape as z, since\n        the sigmoid is computed element-wise on z.\n        \n    Instructions\n    ------------\n    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n    \"\"\"\n    # convert input to a numpy array\n    z = np.array(z)\n    \n    # You need to return the following variables correctly \n    g = np.zeros(z.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    g = 1 / (1 + np.exp(-z))\n\n\n    # =============================================================\n    return g","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunction(theta, X, y):\n    \"\"\"\n    Compute cost and gradient for logistic regression. \n    \n    Parameters\n    ----------\n    theta : array_like\n        The parameters for logistic regression. This a vector\n        of shape (n+1, ).\n    \n    X : array_like\n        The input dataset of shape (m x n+1) where m is the total number\n        of data points and n is the number of features. We assume the \n        intercept has already been added to the input.\n    \n    y : arra_like\n        Labels for the input. This is a vector of shape (m, ).\n    \n    Returns\n    -------\n    J : float\n        The computed value for the cost function. \n    \n    grad : array_like\n        A vector of shape (n+1, ) which is the gradient of the cost\n        function with respect to theta, at the current values of theta.\n        \n    Instructions\n    ------------\n    Compute the cost of a particular choice of theta. You should set J to \n    the cost. Compute the partial derivatives and set grad to the partial\n    derivatives of the cost w.r.t. each parameter in theta.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(theta.shape)\n\n    # ====================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n    grad = (1 / m) * (h - y).dot(X)\n    \n    \n    # =============================================================\n    return J, grad","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCost(theta,X,y):\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n\n    # ====================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n    \n    return J","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you are done call your `costFunction` using two test cases for  $\\theta$ by executing the next cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"options= {'maxiter': 400}\ntheta = np.zeros(3)\nres = optimize.minimize(costFunction,\n                        theta,\n                        (featuresTrain[:,0:3], labelTrain),\n                        jac=True,\n                        method='TNC',\n                        options=options)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(theta, X):\n    \"\"\"\n    Predict whether the label is 0 or 1 using learned logistic regression.\n    Computes the predictions for X using a threshold at 0.5 \n    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n    \n    Parameters\n    ----------\n    theta : array_like\n        Parameters for logistic regression. A vecotor of shape (n+1, ).\n    \n    X : array_like\n        The data to use for computing predictions. The rows is the number \n        of points to compute predictions, and columns is the number of\n        features.\n\n    Returns\n    -------\n    p : array_like\n        Predictions and 0 or 1 for each row in X. \n    \n    Instructions\n    ------------\n    Complete the following code to make predictions using your learned \n    logistic regression parameters.You should set p to a vector of 0's and 1's    \n    \"\"\"\n    m = X.shape[0] # Number of training examples\n\n    # You need to return the following variables correctly\n    p = np.zeros(m)\n\n    # ====================== YOUR CODE HERE ======================\n    p = np.round(sigmoid(X.dot(theta.T)))\n\n    \n    # ============================================================\n    return p","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After you have completed the code in `predict`, we proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Compute accuracy on our training set\np = predict(res.x, featuresTest[:,0:3])\nprint('Train Accuracy: {:.2f} %'.format(np.mean(p == labelTest) * 100))\n","execution_count":36,"outputs":[{"output_type":"stream","text":"Train Accuracy: 95.00 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2 Regularized logistic regression\n\nIn this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nSuppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n\nFirst, we load the data from a CSV file:"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section5\"></a>\n### 2.3 Cost function and gradient\n\nNow you will implement code to compute the cost function and gradient for regularized logistic regression. Complete the code for the function `costFunctionReg` below to return the cost and gradient.\n\nRecall that the regularized cost function in logistic regression is\n\n$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n\nNote that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$\n<a id=\"costFunctionReg\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunctionReg(theta, X, y, lambda_):\n    \"\"\"\n    Compute cost and gradient for logistic regression with regularization.\n    \n    Parameters\n    ----------\n    theta : array_like\n        Logistic regression parameters. A vector with shape (n, ). n is \n        the number of features including any intercept. If we have mapped\n        our initial features into polynomial features, then n is the total \n        number of polynomial features. \n    \n    X : array_like\n        The data set with shape (m x n). m is the number of examples, and\n        n is the number of features (after feature mapping).\n    \n    y : array_like\n        The data labels. A vector with shape (m, ).\n    \n    lambda_ : float\n        The regularization parameter. \n    \n    Returns\n    -------\n    J : float\n        The computed value for the regularized cost function. \n    \n    grad : array_like\n        A vector of shape (n, ) which is the gradient of the cost\n        function with respect to theta, at the current values of theta.\n    \n    Instructions\n    ------------\n    Compute the cost `J` of a particular choice of theta.\n    Compute the partial derivatives and set `grad` to the partial\n    derivatives of the cost w.r.t. each parameter in theta.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(theta.shape)\n\n    # ===================== YOUR CODE HERE ======================\n    h = sigmoid(X.dot(theta.T))\n    \n    temp = theta\n    temp[0] = 0\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n    \n    grad = (1 / m) * (h - y).dot(X) \n    grad = grad + (lambda_ / m) * temp\n    # =============================================================\n    return J, grad","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCostReg(theta,X,y,lambda_):\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n\n    h = sigmoid(X.dot(theta.T))\n    \n    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(theta))\n    \n    return J","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lambdas = [0,0.01,0.02,0.04,0.08,0.16,0.32,0.64]\noptions= {'maxiter': 100}\n\nfor i in range(len(lambdas)):\n    initial_theta = np.zeros(5)\n    res = optimize.minimize(costFunctionReg,\n                            initial_theta,\n                            (featuresTrain[:,0:5], labelTrain, lambdas[i]),\n                            jac=True,\n                            method='TNC',\n                            options=options)\n    print(computeCostReg(res.x,featuresCross[:,0:5],labelCross,lambdas[i]))\n\n","execution_count":40,"outputs":[{"output_type":"stream","text":"cost is equal =  0.31665125717643794\ncost is equal =  0.316659337419932\ncost is equal =  0.3166678020221888\ncost is equal =  0.3166848705239626\ncost is equal =  0.31671735029959436\ncost is equal =  0.31678510690279366\ncost is equal =  0.3169211745044545\ncost is equal =  0.3171923355236995\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}